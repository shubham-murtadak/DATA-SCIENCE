{
 "cells": [
  {
   "cell_type": "raw",
   "id": "83034a7e",
   "metadata": {},
   "source": [
    "7) Text Analytics\n",
    "1. Extract Sample document and apply following document preprocessing methods:\n",
    "Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization.\n",
    "2. Create representation of documents by calculating Term Frequency and Inverse\n",
    "DocumentFrequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5d17ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\" \n",
    "My Name Is Shubham .\n",
    "I am from Shirdi ,Ahmadnagar,Maharashtra.\n",
    "At Present I am living at Shivajinagar Pune.\n",
    "Currently I am persuing My bachelors Degree In Artificial Inetelligence And Data Science.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65818b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "My Name Is Shubham .\n",
      "I am from Shirdi ,Ahmadnagar,Maharashtra.\n",
      "At Present I am living at Shivajinagar Pune.\n",
      "Currently I am persuing My bachelors Degree In Artificial Inetelligence And Data Science.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6d9174",
   "metadata": {},
   "source": [
    "## A.TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fe1ce5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af7a1d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens=word_tokenize(text)\n",
    "sent_tokens=sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e8071c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'Name', 'Is', 'Shubham', '.', 'I', 'am', 'from', 'Shirdi', ',', 'Ahmadnagar', ',', 'Maharashtra', '.', 'At', 'Present', 'I', 'am', 'living', 'at', 'Shivajinagar', 'Pune', '.', 'Currently', 'I', 'am', 'persuing', 'My', 'bachelors', 'Degree', 'In', 'Artificial', 'Inetelligence', 'And', 'Data', 'Science', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afd2b22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' \\nMy Name Is Shubham .', 'I am from Shirdi ,Ahmadnagar,Maharashtra.', 'At Present I am living at Shivajinagar Pune.', 'Currently I am persuing My bachelors Degree In Artificial Inetelligence And Data Science.']\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffc24e7",
   "metadata": {},
   "source": [
    "## B.POS TAGGING"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5fc05572",
   "metadata": {},
   "source": [
    "POS Tagging stands for Part-of-Speech Tagging, which involves assigning a part-of-speech tag to each token in a text. This helps in understanding the grammatical structure of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1073671e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07e2d58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\SHUBHAM MURTADAK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b6d07d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags=pos_tag(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7e0994f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('My', 'PRP$'), ('Name', 'NN'), ('Is', 'VBZ'), ('Shubham', 'NNP'), ('.', '.'), ('I', 'PRP'), ('am', 'VBP'), ('from', 'IN'), ('Shirdi', 'NNP'), (',', ','), ('Ahmadnagar', 'NNP'), (',', ','), ('Maharashtra', 'NNP'), ('.', '.'), ('At', 'IN'), ('Present', 'NNP'), ('I', 'PRP'), ('am', 'VBP'), ('living', 'VBG'), ('at', 'IN'), ('Shivajinagar', 'NNP'), ('Pune', 'NNP'), ('.', '.'), ('Currently', 'NNP'), ('I', 'PRP'), ('am', 'VBP'), ('persuing', 'VBG'), ('My', 'PRP$'), ('bachelors', 'NNS'), ('Degree', 'NNP'), ('In', 'IN'), ('Artificial', 'NNP'), ('Inetelligence', 'NNP'), ('And', 'CC'), ('Data', 'NNP'), ('Science', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e9010b3d",
   "metadata": {},
   "source": [
    "'My' is tagged as 'PRP$', which stands for possessive pronoun.\n",
    "'Name' is tagged as 'NN', which stands for noun, singular or mass.\n",
    "'Is' is tagged as 'VBZ', which stands for verb, 3rd person singular present.\n",
    "'Shubham' is tagged as 'NNP', which stands for proper noun, singular.\n",
    "'.' is tagged as '.', which denotes punctuation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0bd23d",
   "metadata": {},
   "source": [
    "## 3.Removing stopwords"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d0afe685",
   "metadata": {},
   "source": [
    "Stopwords are common words like “the,” “is,” and “and” that often occur frequently but convey little semantic meaning. Removing stopwords can improve the efficiency of text analysis by reducing noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2939da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d8c9994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    #tokenize the text\n",
    "    tokens=word_tokenize(text)\n",
    "    \n",
    "    #get english stop words\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    \n",
    "    #Remove stop words from tokens\n",
    "    filtered_tokens=[word for word in tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    #Join the filtered tokens back into a sentence\n",
    "    filtered_text=' '.join(filtered_tokens)\n",
    "    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ab2e347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "My Name Is Shubham .\n",
      "I am from Shirdi ,Ahmadnagar,Maharashtra.\n",
      "At Present I am living at Shivajinagar Pune.\n",
      "Currently I am persuing My bachelors Degree In Artificial Inetelligence And Data Science.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9cd23b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_text=remove_stopwords(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f64c31d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name Shubham . Shirdi , Ahmadnagar , Maharashtra . Present living Shivajinagar Pune . Currently persuing bachelors Degree Artificial Inetelligence Data Science .\n"
     ]
    }
   ],
   "source": [
    "print(filtered_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a135288",
   "metadata": {},
   "source": [
    "# 4.stemming"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eefd9c19",
   "metadata": {},
   "source": [
    "It is also known as the text standardization step where the words are stemmed or diminished to their root/base form.  For example, words like ‘programmer’, ‘programming, ‘program’ will be stemmed to ‘program’.\n",
    "\n",
    "But the disadvantage of stemming is that it stems the words such that its root form loses the meaning or it is not diminished to a proper English word. We will see this in the steps done below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c88676a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "31d120a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f32daf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2dbd4fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My -> my\n",
      "Name -> name\n",
      "Is -> is\n",
      "Shubham -> shubham\n",
      ". -> .\n",
      "I -> i\n",
      "am -> am\n",
      "from -> from\n",
      "Shirdi -> shirdi\n",
      ", -> ,\n",
      "Ahmadnagar -> ahmadnagar\n",
      ", -> ,\n",
      "Maharashtra -> maharashtra\n",
      ". -> .\n",
      "At -> at\n",
      "Present -> present\n",
      "I -> i\n",
      "am -> am\n",
      "living -> live\n",
      "at -> at\n",
      "Shivajinagar -> shivajinagar\n",
      "Pune -> pune\n",
      ". -> .\n",
      "Currently -> current\n",
      "I -> i\n",
      "am -> am\n",
      "persuing -> persu\n",
      "My -> my\n",
      "bachelors -> bachelor\n",
      "Degree -> degre\n",
      "In -> in\n",
      "Artificial -> artifici\n",
      "Inetelligence -> inetellig\n",
      "And -> and\n",
      "Data -> data\n",
      "Science -> scienc\n",
      ". -> .\n"
     ]
    }
   ],
   "source": [
    "#perform stemming on each word\n",
    "for i in tokens:\n",
    "    print(i,\"->\",porter_stemmer.stem(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c538120",
   "metadata": {},
   "source": [
    "# 5.Lemmitization"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8a8d3479",
   "metadata": {},
   "source": [
    "It stems the word but makes sure that it does not lose its meaning.  Lemmatization has a pre-defined dictionary that stores the context of words and checks the word in the dictionary while diminishing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "703e5881",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8c2cc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dbaf1f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My -> My\n",
      "Name -> Name\n",
      "Is -> Is\n",
      "Shubham -> Shubham\n",
      ". -> .\n",
      "I -> I\n",
      "am -> am\n",
      "from -> from\n",
      "Shirdi -> Shirdi\n",
      ", -> ,\n",
      "Ahmadnagar -> Ahmadnagar\n",
      ", -> ,\n",
      "Maharashtra -> Maharashtra\n",
      ". -> .\n",
      "At -> At\n",
      "Present -> Present\n",
      "I -> I\n",
      "am -> am\n",
      "living -> living\n",
      "at -> at\n",
      "Shivajinagar -> Shivajinagar\n",
      "Pune -> Pune\n",
      ". -> .\n",
      "Currently -> Currently\n",
      "I -> I\n",
      "am -> am\n",
      "persuing -> persuing\n",
      "My -> My\n",
      "bachelors -> bachelor\n",
      "Degree -> Degree\n",
      "In -> In\n",
      "Artificial -> Artificial\n",
      "Inetelligence -> Inetelligence\n",
      "And -> And\n",
      "Data -> Data\n",
      "Science -> Science\n",
      ". -> .\n"
     ]
    }
   ],
   "source": [
    "for i in tokens:\n",
    "    print(i,\"->\",lemmatizer.lemmatize(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476466db",
   "metadata": {},
   "source": [
    "# 2. Create representation of documents by calculating Term Frequency and Inverse  Document Frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d5cfad6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "11d485d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2b2dc894",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize TF-IDF vectorizer\n",
    "tfidf_vectorizer=TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "651fe114",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the  vectorizer to the documents and transform the documents into TF-IDF matrix\n",
    "tfidf_matrix=tfidf_vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c71924b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the feature names \n",
    "feature_names=tfidf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5a4f230e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix :\n",
      "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]\n",
      " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
      "  0.28108867 0.         0.28108867]\n",
      " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
      "  0.26710379 0.51184851 0.26710379]\n",
      " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "print(\"TF-IDF Matrix :\")\n",
    "print(feature_names)\n",
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2e96a0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names :\n",
      "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature names :\")\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f05a8d0",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb892043",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
